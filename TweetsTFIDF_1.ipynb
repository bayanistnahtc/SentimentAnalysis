{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> First Approach: TF/IDF </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocessing ---\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# --- Processing ---\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- Postprocessing ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: Index(['sentiment', 'content'], dtype='object')\n",
      "Train length: 30000\n",
      "Test length: 10000 \n"
     ]
    }
   ],
   "source": [
    "# Reading the files\n",
    "train = pd.read_csv('Datasets/1train_data.csv',encoding='ISO-8859-1')\n",
    "test = pd.read_csv('Datasets/1test_data.csv',encoding='ISO-8859-1')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "print('Column names: %s'%(train.columns))\n",
    "print('Train length: %s'%(len(train)))\n",
    "print('Test length: %s '%(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>happiness</td>\n",
       "      <td>I had a great date last night...tried to find the CDCaves with Daniel  it was HILARIOUSLY FUN!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>sadness</td>\n",
       "      <td>With alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>happiness</td>\n",
       "      <td>@fureousangel that is comedy  good luck my friend!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>fun</td>\n",
       "      <td>stephs grad party gr8! shoved cake in her face, watchd sis bitch slap a boy, ate good food  satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>happiness</td>\n",
       "      <td>@jesfive SWEEEEET - San Fran is awesome!!!!  Love it there</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment  \\\n",
       "29995  happiness   \n",
       "29996  sadness     \n",
       "29997  happiness   \n",
       "29998  fun         \n",
       "29999  happiness   \n",
       "\n",
       "                                                                                                     content  \n",
       "29995  I had a great date last night...tried to find the CDCaves with Daniel  it was HILARIOUSLY FUN!!!       \n",
       "29996  With alex                                                                                              \n",
       "29997  @fureousangel that is comedy  good luck my friend!                                                     \n",
       "29998  stephs grad party gr8! shoved cake in her face, watchd sis bitch slap a boy, ate good food  satisfied  \n",
       "29999  @jesfive SWEEEEET - San Fran is awesome!!!!  Love it there                                             "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>happiness</td>\n",
       "      <td>i had a great date last nighttried to find the cdcaves with daniel  it was hilariously fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>sadness</td>\n",
       "      <td>with alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>happiness</td>\n",
       "      <td>that is comedy  good luck my friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>fun</td>\n",
       "      <td>stephs grad party gr8 shoved cake in her face watchd sis bitch slap a boy ate good food  satisfied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>happiness</td>\n",
       "      <td>sweeeeet  san fran is awesome  love it there</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment  \\\n",
       "29995  happiness   \n",
       "29996  sadness     \n",
       "29997  happiness   \n",
       "29998  fun         \n",
       "29999  happiness   \n",
       "\n",
       "                                                                                                  content  \n",
       "29995  i had a great date last nighttried to find the cdcaves with daniel  it was hilariously fun          \n",
       "29996  with alex                                                                                           \n",
       "29997   that is comedy  good luck my friend                                                                \n",
       "29998  stephs grad party gr8 shoved cake in her face watchd sis bitch slap a boy ate good food  satisfied  \n",
       "29999   sweeeeet  san fran is awesome  love it there                                                       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tweets cleaning\n",
    "def clean_text(df,text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    mystring = r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"\n",
    "    # the first filter is capture everything eclosed that starts with an @\n",
    "    # and that has any single character from the range A-Z or a-z or 0-9 with more\n",
    "    # than one element inside of that\n",
    "    # the second filter is capture everything that is not a number, a lower or upper\n",
    "    # case letter after the last line\n",
    "    df[text_field] = df[text_field].apply(lambda element: re.sub(mystring,\"\",element))\n",
    "    return df\n",
    "\n",
    "train_clean = clean_text(train,'content')\n",
    "test_clean = clean_text(test,'content')\n",
    "train_clean.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = [k for k in train_clean['content'] if k.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT connected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/bert_serving/client/__init__.py:286: UserWarning: some of your sentences have more tokens than \"max_seq_len=25\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    }
   ],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "print('BERT connected')\n",
    "data = bc.encode(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('bert_emb_1train_data.pk', mode='wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_emb_1train_data.pk', mode='rb') as f:\n",
    "    loaded_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29938, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train length: %s'%(len(train)))\n",
    "print('Test length: %s '%(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = list(set(train_clean['sentiment']))\n",
    "how_many_sentiments = len(sentiments)\n",
    "print('Sentiment labels: %s \\n\\nNumber of sentiments: %s '%(sentiments,how_many_sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_clean['target'] = le.fit_transform(train_clean['sentiment'])\n",
    "train_clean.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Upsampling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-racist/non-sexist (0's) tweets\n",
    "# in the training\n",
    "lista = [train_clean[train_clean.target==index] for index in list(set(train_clean['target']))]\n",
    "lista_dos = [len(k) for k in lista]\n",
    "mymap = dict(zip(le.classes_,lista_dos))\n",
    "print('Number of non-racist/non-sexist tweets: \\n%s '%(mymap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of racist/sexist (1's) tweets in the training\n",
    "train_minority = train_clean[train_clean.label==1]\n",
    "print('Number of racist/sexist tweets: %s '%(len(train_minority)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling:\n",
    "# We use the tags with less frequency, in this case 1's, inside\n",
    "# the training in order to create an upsampled training set with\n",
    "# the same dimension the tags with the highest frequency have (in\n",
    "# this case 0's), allowing tweet repetition.\n",
    "train_minority_upsampled = resample(train_minority, # data  set to use\n",
    "                                    replace=True, # repetitions are allowed\n",
    "                                    n_samples=len(train_majority), # I want 29720 samples\n",
    "                                    random_state=123) # using this random state\n",
    "\n",
    "# Now that the minority of tweets is no longer outmatched by \n",
    "# the majority, we can put them together, obtaining a training\n",
    "# set of twice the size of the majority, i,.e. 2*(2970) = 59440\n",
    "train_upsampled = pd.concat([train_minority_upsampled, train_majority])\n",
    "print('Upsampling')\n",
    "print('Number of total tweets in training: %s,%s '%((train_upsampled.shape)))\n",
    "print(train_upsampled['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Downsampling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-racist/non-sexist (0's) tweets\n",
    "# in the training\n",
    "train_majority = train_clean[train_clean.label==0]\n",
    "print('Number of non-racist/non-sexist tweets: %s '%(len(train_majority)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of racist/sexist (1's) tweets in the training\n",
    "train_minority = train_clean[train_clean.label==1]\n",
    "print('Number of non-racist/non-sexist tweets: %s '%(len(train_minority)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling:\n",
    "# We use the tags with the largest frequency, in this case the \n",
    "# 0's inside the training in order to create an downsampled training \n",
    "# set with the same dimension the tags with the lowest frequency have (in\n",
    "# this case 1's), allowing tweet repetition.\n",
    "train_majority_downsampled = resample(train_majority, # use the train majority\n",
    "                                 replace=True,  # repetitions are allowed\n",
    "                                 n_samples=len(train_minority), # I want 2242 samples\n",
    "                                 random_state=123) #using this random state\n",
    "\n",
    "# Now that the majority of tweets has been reduced to a set\n",
    "# with the same size as the minority, we can put them together,\n",
    "# obtaining a training set of twice the size of the minority, i,.e. 2*(2242) = 4484\n",
    "train_downsampled = pd.concat([train_majority_downsampled, train_minority])\n",
    "print('Downsampling')\n",
    "print('Number of total tweets in training: %s,%s '%((train_downsampled.shape)))\n",
    "print(train_downsampled['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model Training </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_upsampled['tweet'],\n",
    "                                                    train_upsampled['label'],\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Format: Training\n",
    "#\n",
    "# 1. Vectorizer \n",
    "# Here we get the list of words of the whole \n",
    "# training set, the whole tokenized version \n",
    "# of the dataset.\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# 2. TF/IDF\n",
    "# It indicates how many times a word appears\n",
    "# inside each tweet.\n",
    "tfidf_transformer = TfidfTransformer()#use_idf=False).fit(X)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X)\n",
    "\n",
    "# The Model\n",
    "clf = SGDClassifier()\n",
    "model = SGDClassifier().fit(X_train_tfidf,y_train)\n",
    "\n",
    "# Format: Testing\n",
    "# \n",
    "# 1. Vectorizer \n",
    "X_prime = vectorizer.transform(X_test)\n",
    "# 2. TF/IDF\n",
    "X_test_tfidf = tfidf_transformer.transform(X_prime)\n",
    "\n",
    "# Prediction\n",
    "y_predict = model.predict(X_test_tfidf)\n",
    "f1_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_predict))#,target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pipeline </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines:\n",
    "pipeline_sgd = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('nb', SGDClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prediction (Upsampling) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Training Splitting\n",
    "X_train_up, X_test_up, y_train_up, y_test_up = train_test_split(train_upsampled['tweet'],\n",
    "                                                    train_upsampled['label'],\n",
    "                                                    random_state=0)\n",
    "# Model\n",
    "model_up = pipeline_sgd.fit(X_train_up, y_train_up)\n",
    "model_up\n",
    "\n",
    "y_predict_up = model_up.predict(X_test_up)\n",
    "y_predict_up\n",
    "print('Accuracy with upsampling: %s '%(f1_score(y_test_up, y_predict_up)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prediction (Downsampling) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Training Splitting\n",
    "X_train_down, X_test_down, y_train_down, y_test_down = train_test_split(train_downsampled['tweet'],\n",
    "                                                    train_downsampled['label'],\n",
    "                                                    random_state=0)\n",
    "# Model\n",
    "model_down = pipeline_sgd.fit(X_train_down, y_train_down)\n",
    "model_down\n",
    "\n",
    "y_predict_down = model_up.predict(X_test_down)\n",
    "y_predict_down\n",
    "print('Accuracy with downsampling: %s '%(f1_score(y_test_down, y_predict_down)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scikit-learn bootstrap\n",
    "# from sklearn.utils import resample\n",
    "# # data sample\n",
    "# data = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "# print('data: %s'%(data))\n",
    "# # prepare bootstrap sample\n",
    "# boot = resample(data, replace=True, n_samples=2*len(data), random_state=1)\n",
    "# print('Bootstrap Sample: %s' % boot)\n",
    "# # out of bag observations\n",
    "# oob = [x for x in data if x not in boot]\n",
    "# # print('OOB Sample: %s' % oob)\n",
    "\n",
    "# # data: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "# # Bootstrap Sample: [0.6, 0.4, 0.5, 0.1, 0.2, 0.4, 0.6, 0.1, 0.1, 0.2, 0.5, 0.6]\n",
    "# # OOB Sample: [0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myvectorizer = CountVectorizer()\n",
    "myX = myvectorizer.fit_transform(corpus)\n",
    "print(myvectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']\n",
    "# ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "# [[0 1 1 1 0 0 1 0 1]\n",
    "#  [0 2 0 1 0 1 1 0 1]\n",
    "#  [1 0 0 1 1 0 1 1 1]\n",
    "#  [0 1 1 1 0 0 1 0 1]]\n",
    "\n",
    "mytf_transformer = TfidfTransformer(use_idf=False).fit(myX)\n",
    "print(mytf_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myX_train_tf = mytf_transformer.transform(myX)\n",
    "print(myX_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myX_train_tf.toarray()\n",
    "# TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False,\n",
    "#          use_idf=False)\n",
    "#   (0, 1)\t0.4472135954999579\n",
    "#   (0, 2)\t0.4472135954999579\n",
    "#   (0, 3)\t0.4472135954999579\n",
    "#   (0, 6)\t0.4472135954999579\n",
    "#   (0, 8)\t0.4472135954999579\n",
    "#   (1, 1)\t0.7071067811865475\n",
    "#   (1, 3)\t0.35355339059327373\n",
    "#   (1, 5)\t0.35355339059327373\n",
    "#   (1, 6)\t0.35355339059327373\n",
    "#   (1, 8)\t0.35355339059327373\n",
    "#   (2, 0)\t0.4082482904638631\n",
    "#   (2, 3)\t0.4082482904638631\n",
    "#   (2, 4)\t0.4082482904638631\n",
    "#   (2, 6)\t0.4082482904638631\n",
    "#   (2, 7)\t0.4082482904638631\n",
    "#   (2, 8)\t0.4082482904638631\n",
    "#   (3, 1)\t0.4472135954999579\n",
    "#   (3, 2)\t0.4472135954999579\n",
    "#   (3, 3)\t0.4472135954999579\n",
    "#   (3, 6)\t0.4472135954999579\n",
    "#   (3, 8)\t0.4472135954999579\n",
    "# (4, 9)\n",
    "\n",
    "# myclf = SGDClassifier()#loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
